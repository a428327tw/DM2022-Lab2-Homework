{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Data preparation\n",
    "2. Feature engineering\n",
    "3. Model architecture\n",
    "4. Training & testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "Load the json and csv files to each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_json(\"dm2022-isa5810-lab2-homework2/tweets_DM.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = pd.read_csv(\"dm2022-isa5810-lab2-homework2/data_identification.csv\") #, index_col='tweet_id')\n",
    "emotion = pd.read_csv(\"dm2022-isa5810-lab2-homework2/emotion.csv\")#, index_col='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1867535, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_id.shape)\n",
    "data_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only \"tweet_id\" and \"text\" from the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = [tweets['_source'][i]['tweet']['tweet_id'] for i in range(len(tweets['_source']))]\n",
    "tweet_text = [tweets['_source'][i]['tweet']['text'] for i in range(len(tweets['_source']))]\n",
    "tweet = pd.DataFrame({'tweet_id': tweet_id, 'text': tweet_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge three dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_id.merge(emotion, on='tweet_id', how='outer').merge(tweet, on='tweet_id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe into training / test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['identification']=='train']\n",
    "test_df = df[df['identification']=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training dataframe into training / validation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = train_df.sample(frac=0.2)\n",
    "train_df = train_df[~train_df.index.isin(val_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"./train_df.pkl\") \n",
    "test_df.to_pickle(\"./test_df.pkl\")\n",
    "val_df.to_pickle(\"./val_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I choose \"Bag of Words\" which achieved the better perform\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Coo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "BOW500_vectorizer = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply vectorizer to the text of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BOW500_vectorizer \u001b[39m=\u001b[39m CountVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, tokenizer\u001b[39m=\u001b[39mnltk\u001b[39m.\u001b[39mword_tokenize) \n\u001b[0;32m      4\u001b[0m \u001b[39m# apply analyzer to training data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m BOW500_vectorizer\u001b[39m.\u001b[39mfit(train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# apply analyzer to training data\n",
    "BOW500_vectorizer.fit(train_df['text'])\n",
    "\n",
    "# Transform documents to matrix.\n",
    "train_data_BOW_features_500 = BOW500_vectorizer.transform(train_df['text'])\n",
    "\n",
    "## Check dimension\n",
    "train_data_BOW_features_500.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set the training data and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 500)\n",
      "y_train.shape:  (1164450,)\n",
      "X_val.shape:  (291113, 500)\n",
      "y_val.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW500_vectorizer.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_val = BOW500_vectorizer.transform(val_df['text'])\n",
    "y_val = val_df['emotion']# all of this sould be nan\n",
    "\n",
    "## take a look at data dimension is a good habit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['emotion']\n",
    "y_val = val_df['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to transform our categorical  labels to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1             joy\n",
      "4           trust\n",
      "6    anticipation\n",
      "7    anticipation\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_val.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_val.shape:  (291113, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coo\\AppData\\Local\\Temp\\ipykernel_16688\\274867222.py:10: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print('y_train[0:4]:\\n', y_train[0:4])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "## deal with label (string -> one-hot)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils # keras==2.4.0 and tensorflow==2.3.0\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_val = label_encode(label_encoder, y_val)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input/output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                32064     \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " softmax_1 (Softmax)         (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,744\n",
      "Trainable params: 36,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create logger and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "36390/36390 [==============================] - 69s 2ms/step - loss: 1.4207 - accuracy: 0.4816 - val_loss: 1.3877 - val_accuracy: 0.4924\n",
      "Epoch 2/25\n",
      "36390/36390 [==============================] - 65s 2ms/step - loss: 1.3790 - accuracy: 0.4961 - val_loss: 1.3774 - val_accuracy: 0.4978\n",
      "Epoch 3/25\n",
      "36390/36390 [==============================] - 66s 2ms/step - loss: 1.3679 - accuracy: 0.5004 - val_loss: 1.3723 - val_accuracy: 0.4992\n",
      "Epoch 4/25\n",
      "36390/36390 [==============================] - 69s 2ms/step - loss: 1.3610 - accuracy: 0.5029 - val_loss: 1.3719 - val_accuracy: 0.5002\n",
      "Epoch 5/25\n",
      "36390/36390 [==============================] - 75s 2ms/step - loss: 1.3569 - accuracy: 0.5047 - val_loss: 1.3677 - val_accuracy: 0.5022\n",
      "Epoch 6/25\n",
      "36390/36390 [==============================] - 97s 3ms/step - loss: 1.3540 - accuracy: 0.5060 - val_loss: 1.3691 - val_accuracy: 0.5012\n",
      "Epoch 7/25\n",
      "36390/36390 [==============================] - 100s 3ms/step - loss: 1.3514 - accuracy: 0.5070 - val_loss: 1.3688 - val_accuracy: 0.5020\n",
      "Epoch 8/25\n",
      "36390/36390 [==============================] - 104s 3ms/step - loss: 1.3495 - accuracy: 0.5077 - val_loss: 1.3658 - val_accuracy: 0.5025\n",
      "Epoch 9/25\n",
      "36390/36390 [==============================] - 110s 3ms/step - loss: 1.3477 - accuracy: 0.5083 - val_loss: 1.3744 - val_accuracy: 0.5012\n",
      "Epoch 10/25\n",
      "36390/36390 [==============================] - 110s 3ms/step - loss: 1.3468 - accuracy: 0.5087 - val_loss: 1.3699 - val_accuracy: 0.5006\n",
      "Epoch 11/25\n",
      "36390/36390 [==============================] - 113s 3ms/step - loss: 1.3454 - accuracy: 0.5092 - val_loss: 1.3673 - val_accuracy: 0.5018\n",
      "Epoch 12/25\n",
      "36390/36390 [==============================] - 118s 3ms/step - loss: 1.3446 - accuracy: 0.5098 - val_loss: 1.3698 - val_accuracy: 0.5016\n",
      "Epoch 13/25\n",
      "36390/36390 [==============================] - 108s 3ms/step - loss: 1.3438 - accuracy: 0.5101 - val_loss: 1.3739 - val_accuracy: 0.5010\n",
      "Epoch 14/25\n",
      "36390/36390 [==============================] - 112s 3ms/step - loss: 1.3432 - accuracy: 0.5105 - val_loss: 1.3697 - val_accuracy: 0.5018\n",
      "Epoch 15/25\n",
      "36390/36390 [==============================] - 122s 3ms/step - loss: 1.3424 - accuracy: 0.5107 - val_loss: 1.3735 - val_accuracy: 0.5026\n",
      "Epoch 16/25\n",
      "36390/36390 [==============================] - 121s 3ms/step - loss: 1.3419 - accuracy: 0.5107 - val_loss: 1.3703 - val_accuracy: 0.5028\n",
      "Epoch 17/25\n",
      "36390/36390 [==============================] - 129s 3ms/step - loss: 1.3414 - accuracy: 0.5112 - val_loss: 1.3721 - val_accuracy: 0.5017\n",
      "Epoch 18/25\n",
      "36390/36390 [==============================] - 127s 3ms/step - loss: 1.3410 - accuracy: 0.5111 - val_loss: 1.3729 - val_accuracy: 0.5008\n",
      "Epoch 19/25\n",
      "36390/36390 [==============================] - 133s 4ms/step - loss: 1.3409 - accuracy: 0.5111 - val_loss: 1.3712 - val_accuracy: 0.5023\n",
      "Epoch 20/25\n",
      "36390/36390 [==============================] - 132s 4ms/step - loss: 1.3402 - accuracy: 0.5115 - val_loss: 1.3772 - val_accuracy: 0.5005\n",
      "Epoch 21/25\n",
      "36390/36390 [==============================] - 132s 4ms/step - loss: 1.3401 - accuracy: 0.5115 - val_loss: 1.3739 - val_accuracy: 0.5018\n",
      "Epoch 22/25\n",
      "36390/36390 [==============================] - 138s 4ms/step - loss: 1.3398 - accuracy: 0.5118 - val_loss: 1.3748 - val_accuracy: 0.5013\n",
      "Epoch 23/25\n",
      "36390/36390 [==============================] - 138s 4ms/step - loss: 1.3397 - accuracy: 0.5120 - val_loss: 1.3752 - val_accuracy: 0.5006\n",
      "Epoch 24/25\n",
      "36390/36390 [==============================] - 149s 4ms/step - loss: 1.3394 - accuracy: 0.5120 - val_loss: 1.3785 - val_accuracy: 0.4992\n",
      "Epoch 25/25\n",
      "36390/36390 [==============================] - 133s 4ms/step - loss: 1.3403 - accuracy: 0.5117 - val_loss: 1.3783 - val_accuracy: 0.5011\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('./training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_val, y_val))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = BOW500_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12875/12875 [==============================] - 19s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the output vector into each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_test_pred = label_decode(label_encoder, y_test_pred)\n",
    "test_df['emotion'] = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Save output to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"tweet_id\", \"emotion\"]\n",
    "test_df.to_csv('output_NN1.csv', columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5927e545d743d43a7522178c6a3d273ff54047d6f6e976ab10f850edc14cf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
